import cv2
from ultralytics import YOLO

# Load YOLOv8 model
model = YOLO("yolov8n.pt")  # Replace with your model file if different

# Initialize webcam
cap = cv2.VideoCapture(0)

# Define regions for position-based commands
def get_position_and_command(x, frame_width):
    if x < frame_width // 3:
        return "Left", "Move right"
    elif x > 2 * frame_width // 3:
        return "Right", "Move left"
    else:
        return "Center", "Stop and assess"

while True:
    # Read frame from the webcam
    ret, frame = cap.read()
    if not ret:
        break

    # Get frame dimensions
    frame_height, frame_width, _ = frame.shape

    # Perform object detection
    results = model(frame)

    # Draw bounding boxes, label detected objects, and show commands
    for detection in results[0].boxes.data:  # Iterate over detected boxes
        x1, y1, x2, y2 = map(int, detection[:4])  # Get bounding box coordinates
        obj_class = int(detection[5])  # Get object class index
        confidence = detection[4]  # Get confidence score

        # Get object name using YOLOv8's class names
        object_name = model.names[obj_class]

        # Determine the position (left, center, or right) based on x-coordinate
        object_x_center = (x1 + x2) // 2
        position, command = get_position_and_command(object_x_center, frame_width)

        # Draw bounding box on the frame
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)

        # Display object name, position, and command on the frame
        label = f"{object_name} detected: {position} - {command}"
        cv2.putText(frame, label, (x1, y1 - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2, cv2.LINE_AA)

    # Show the video feed with detections and instructions
    cv2.imshow("Object Detection Feed", frame)

    # Break loop on 'q' key press
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release the webcam and close all windows
cap.release()
cv2.destroyAllWindows()
